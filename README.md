# llm-student-misconceptions
This project explores how effectively large language models (LLMs) can evaluate student short-answer responses in an educational setting.

We compared outputs from open source models like Llama3.2 and Phi4 to instructor-assigned grades and annotated misconceptions. The project highlights the limitations and potential of LLMs in automated assessment.

Due to privacy regulations, original student data is not included. All examples in this repository are synthetic.

## Contents
- evaluation_code: Google Colab notebook used in analysis
- LuedekeMisconceptions: Research paper draft and project summary

## Author
Ella Luedeke
